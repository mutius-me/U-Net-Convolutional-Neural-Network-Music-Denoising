{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, Concatenate, ZeroPadding2D\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are correctly aligned.\n",
      "Shape of noisy data matrix: (5868, 1025, 173, 2)\n",
      "Shape of clean data matrix: (5868, 1025, 173, 2)\n"
     ]
    }
   ],
   "source": [
    "def load_spectrograms_into_matrix(directory, print_paths=False):\n",
    "    # Resolve the absolute path of the directory to handle relative paths correctly\n",
    "    abs_directory = os.path.abspath(directory)\n",
    "    data_matrix = []\n",
    "    file_paths = []  # Store file paths for verification\n",
    "\n",
    "    # Walk through the directory structure\n",
    "    for root, dirs, files in sorted(os.walk(abs_directory)):\n",
    "        dirs.sort()  # Sort directories to maintain consistent order\n",
    "        files.sort()  # Sort files to ensure consistent ordering within the same directory\n",
    "        for file in files:\n",
    "            if file.endswith('.npy'):\n",
    "                # Generate the full path to the file\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Optionally print file paths for verification\n",
    "                if print_paths:\n",
    "                    print(file_path)\n",
    "                # Load the spectrogram\n",
    "                spectrogram = np.load(file_path)\n",
    "                # Append the spectrogram and file path to their respective lists\n",
    "                data_matrix.append(spectrogram)\n",
    "                file_paths.append(file_path)\n",
    "\n",
    "    # Convert list to a numpy array for easier manipulation later\n",
    "    data_matrix = np.array(data_matrix)\n",
    "    return data_matrix, file_paths\n",
    "\n",
    "def verify_alignment(noisy_paths, clean_paths):\n",
    "    # Verify that each path in the noisy_paths corresponds correctly to the path in the clean_paths\n",
    "    misalignments = []\n",
    "    for noisy, clean in zip(noisy_paths, clean_paths):\n",
    "        # Strip paths to just the file names minus the \"_mixed\" and extensions for comparison\n",
    "        if os.path.splitext(noisy.split('/')[-1].replace('_mixed', ''))[0] != os.path.splitext(clean.split('/')[-1])[0]:\n",
    "            misalignments.append((noisy, clean))\n",
    "\n",
    "    if misalignments:\n",
    "        print(\"Misaligned files:\")\n",
    "        for mis in misalignments:\n",
    "            print(mis)\n",
    "    else:\n",
    "        print(\"All files are correctly aligned.\")\n",
    "\n",
    "# Define relative paths to the noisy and clean directories using the 'count' variable\n",
    "count = 1  # User can change this as needed\n",
    "noisy_directory = f'../../dataset/iteration-{count}/data/mixed/spectrogram-128-frames'\n",
    "clean_directory = f'../../dataset/iteration-{count}/data/clean/spectrogram-128-frames'\n",
    "\n",
    "# Load data and file paths, resolving paths absolutely\n",
    "X_noisy, noisy_paths = load_spectrograms_into_matrix(noisy_directory, print_paths=False)\n",
    "Y_clean, clean_paths = load_spectrograms_into_matrix(clean_directory, print_paths=False)\n",
    "\n",
    "# Verify alignment of loaded data\n",
    "verify_alignment(noisy_paths, clean_paths)\n",
    "\n",
    "# Example of how to use the data\n",
    "print(f\"Shape of noisy data matrix: {X_noisy.shape}\")\n",
    "print(f\"Shape of clean data matrix: {X_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, Concatenate, ZeroPadding2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def make_divisible_by(x, factor):\n",
    "    return factor * ((x + factor - 1) // factor)\n",
    "\n",
    "def pad_to_size(bottom_layer, size):\n",
    "    h_diff = size[0] - bottom_layer.shape[1]\n",
    "    w_diff = size[1] - bottom_layer.shape[2]\n",
    "    padding = ((h_diff // 2, h_diff - h_diff // 2), (w_diff // 2, w_diff - w_diff // 2))\n",
    "    return ZeroPadding2D(padding=padding)(bottom_layer)\n",
    "\n",
    "def unet_model(input_size=(1025, 173, 2)):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    target_size = (make_divisible_by(inputs.shape[1], 16), make_divisible_by(inputs.shape[2], 16))\n",
    "    x = pad_to_size(inputs, target_size)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    # Bottleneck\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    # Decoder\n",
    "    u3 = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(c4)\n",
    "    u3 = Concatenate()([u3, c3])\n",
    "\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(u3)\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    u2 = Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(c5)\n",
    "    u2 = Concatenate()([u2, c2])\n",
    "\n",
    "    c6 = Conv2D(32, (3, 3), activation='relu', padding='same')(u2)\n",
    "    c6 = Conv2D(32, (3, 3), activation='relu', padding='same')(c6)\n",
    "\n",
    "    u1 = Conv2DTranspose(16, (3, 3), strides=(2, 2), padding='same')(c6)\n",
    "    u1 = Concatenate()([u1, c1])\n",
    "\n",
    "    c7 = Conv2D(16, (3, 3), activation='relu', padding='same')(u1)\n",
    "    c7 = Conv2D(16, (3, 3), activation='relu', padding='same')(c7)\n",
    "\n",
    "    # Output\n",
    "    outputs = Conv2D(2, (1, 1), activation='sigmoid')(c7)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Instantiate and compile the model\n",
    "# model = unet_model()\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_model()\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 1025 and 1040 for '{{node compile_loss/mean_squared_error/sub}} = Sub[T=DT_FLOAT](data_1, functional_1_1/conv2d_70_1/Sigmoid)' with input shapes: [?,1025,173,2], [?,1040,176,2].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming X_noisy and Y_clean are already numpy arrays properly shaped ##TODO rename X_clean\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Local/senior-project/myenv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Developer/Local/senior-project/myenv/lib/python3.12/site-packages/keras/src/losses/losses.py:1154\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m   1152\u001b[0m y_true \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_true, dtype\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   1153\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001b[0;32m-> 1154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mmean(ops\u001b[38;5;241m.\u001b[39msquare(\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 1025 and 1040 for '{{node compile_loss/mean_squared_error/sub}} = Sub[T=DT_FLOAT](data_1, functional_1_1/conv2d_70_1/Sigmoid)' with input shapes: [?,1025,173,2], [?,1040,176,2]."
     ]
    }
   ],
   "source": [
    "# Assuming X_noisy and Y_clean are already numpy arrays properly shaped ##TODO rename X_clean\n",
    "history = model.fit(X_noisy, X_clean, batch_size=4, epochs=10, validation_split=0.2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
